---
title: "Whole Genome Sequencing of *Ascochyta rabiei* Isolates (2020)"
author: "Ido Bar"
date: "11 April 2021"
always_allow_html: yes
output: 
    # md_document:
#      css: "style/style.css"
      # toc: true
      # toc_depth: 3
#      highlight: pygments
#      number_sections: false
    html_document:
      css: "style/style.css"
      toc: true
      toc_float: true
      toc_depth: 3
      highlight: pygments
      number_sections: false
      code_folding: hide
#      keep_md: true
bibliography: style/Fungal_genomes.bib
csl: style/springer-basic-improved-author-date-with-italic-et-al-period.csl
---

```{r setup, include=FALSE}
devtools::source_gist("7f63547158ecdbacf31b54a58af0d1cc", filename = "util.R")
knitr::opts_chunk$set(list(echo = TRUE, eval=FALSE, message=FALSE))
# options(width = 180)
# CRAN settings
chooseCRANmirror(ind=1)
options(repos = getOption("repos")["CRAN"]) # fix the annoying "unable to access index..."
pacman::p_load(tidyverse, readxl, scales, paletteer, 
               htmltools, ggrepel, here,  DT, captioner, 
               tabulizer)
# Connect to Zotero to access references
# biblio <- ReadBib("data/Fungal_genomes.bib") # , "bibtex", "RefManageR"
# Font Format
custom_font="consolas"
fontFmt = function(x,font="consolas"){
  #outputFormat = knitr::opts_knit$get("rmarkdown.pandoc.to")
  #if (outputFormat == 'html')
  formatted_text <- sprintf("<font face='%s'>%s</font>",font,x)
  return(formatted_text)
  #else
  #  x
}
```



```{r captions, include=FALSE, eval=TRUE}
figs <- captioner(prefix="Figure")
tbls <- captioner(prefix="Table")
tbls(name="samples",HTML("<i>Ascochyta rabiei</i> samples used in this study."))
tbls(name="mapping_rates", HTML("Mapping rates of the WGS reads to the <i>Ascochyta rabiei</i> Me14 reference genome."))
tbls(name="mapping_sum", "Mapping statistics for 2021 sequencing batches.")
figs(name="GC_cont", "GC (%) content in trimmed WGS reads.")

selected_isolates_file <-
  here("../samples_db/data/Ascochyta_rabiei_200_selection_2020.xlsx")
selected_samples <- read_excel(selected_isolates_file,
                                       sheet = "selected_isolates_Ido") %>% 
  setNames(stringr::str_to_title(sub("_.+$", "", names(.)))) %>% .[c(8:7,1:4)] %>% rename(Host=Genotype)

patho_table <- read_excel("../samples_db/output/A_rabiei_pathotypes_2013-2020.xlsx", sheet = "pathotyping")
sequencing_table <- read_excel("./sample_info/AGRF_NGS_Plates.xlsx", sheet = "Sample Details") %>% 
  mutate(Isolate=sub("_R$", "", `Sample Name`))

samples_table <- selected_samples %>% left_join(patho_table %>% select(Isolate, Year, Pathogenicity=Path_rating)) %>% 
  arrange(Isolate)

#readxl::read_excel("../P_rabiei_isolate_list_for_wgs.xlsx", sheet = "Sequenced") %>%
  # filter(!is.na(Sequenced))  %>% inner_join(isolate_table) %>%
  # mutate(Collection_Year=as.character(isolate_table$Year[match(Isolate, isolate_table$isolate_name)]),
  #        Collection_Year=if_else(!is.na(Collection_Year), Collection_Year, 
  #                         paste0("20", sub("[A-Z]*(\\d{2}).+", "\\1", Isolate)))) %>%
  # dplyr::select(Isolate, Site, State, Collection_Year, Rating, Pathogenicity,Haplotype) %>%
   # %>% write_csv("isolate_list_WGS_2018.csv")
# xlsx::write.xlsx(as.data.frame(samples_table))
# tfam_table <- samples_table  %>% dplyr::select(State, Isolate) %>%
#   mutate(V3=0, V4=0, V5=0, V6=-9)
#figs(name="WtFreq1","Weight frequency of Ruffe captured in 1992.")

```

# Experimental Design
In 2021, DNA was extracted from 197 isolates of _Ascochyta rabiei_ and sent for Whole-Genome-Sequencing (WGS) at the Australian Genome Research Facility (AGRF, Melbourne) on 1 lane of a NovaSeq flowcell, producing 150 bp paired-end reads (run name CAGRF20114478).  
Details of the sequenced isolates is provided in (`r tbls(name="samples",display="cite")`).

```{r samples_table, eval=TRUE} 
datatable(samples_table, caption=tbls("samples"), rownames = FALSE) %>% # , 
          # options = list(dom = 'tf', pageLength = 40)) %>%
  formatStyle('Pathogenicity',
  backgroundColor = styleInterval(0:4, c('limegreen','gold', 'orange', 'orangered', 'firebrick', 'darkred'))
)# pander , justify="left"
```

# Aims
* Identify strain-unique variants to develop detection methods
* Associate aggressiveness with specific variants

# Analysis Pipeline
## General overview:
1. Data pre-processing:
    a. Quality check
    b. Adaptor trimming
    c. Post-trim quality check
2. Mapping reads to a reference genome 
3. Reads deduplication
4. Variant calling and filtration
5. Variant annotation (including assigning SSR haplotypes)
6. Variant-Pathogenicity association
7. Produce variant statistics and assessment 

## Methods
DNA-Seq data processing, mapping and variant calling were performed on the _QCIF Awoonga HPC Cluster_ (using PBSPro scheduler), using [`r fontFmt("Snippy")` v4.6.0](https://github.com/tseemann/snippy), a rapid haploid variant calling and core genome alignment. The pipeline uses `r fontFmt("FreeBayes")` v1.3.5 [@garrison_haplotype-based_2012] and other tools to assign variant probability scores and call variants.  

Consider the following options for file download and processing:  

1. Download and perform all processing on the compute node `$TMPDIR` (limited to 20GB), then cleanup (using `--cleanup` in `r fontFmt("Snippy")`) and copy just the important results (`fastq.genozip` files, QC reports and Snippy results) to scratch folder `(30days` or `90days` on Awoonga) and into CloudStor (with `rclone`)  
2. Download and perform all processing on scratch folder `(30days` or `90days` on Awoonga), then cleanup and upload into CloudStor.  

At the current pipeline, all processing is done at the the read files were downloaded and then following a standard naming conventions (to start from a pair of `SampleID_R#.fastq.gz` file), to make it much easier to process all files with the same script, using parameters to specify batch name, read length, sequencing platform and other potential variables.   
Detailed methods, including code for running each of the analyses steps are provided in the associated [A_rabiei_WGS_analysis GitHub repository](https://github.com/IdoBar/A_rabiei_WGS_analysis/tree/2020_samples).

### Data pre-processing
Install needed software in a `conda` environment on the new Gowonda HPC cluster.
```{bash setup_tools}
# download conda
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
# install conda
bash Miniconda3-latest-Linux-x86_64.sh 
# initialise conda
source ~/.bashrc
# add channels and set priorities
conda config --add channels conda-forge
conda config --append channels bioconda
# install extra packages to the base environment
conda install -n base libgcc gnutls libuuid readline cmake git tmux libgfortran parallel \
  gawk pigz rename genozip autoconf sshpass 
# install snippy (need to fix internet connection to gowonda2 - use patched netcheck in ~/bin)
# source ~/.proxy
CONDA_NAME=snippy
conda create -n $CONDA_NAME snippy sra-tools bcbio-gff libgd xorg-libxpm libpng libjpeg-turbo jpeg \
                libtiff genozip parallel qualimap multiqc bbmap fastp genozip
# Clean extra space
# conda update -n base conda
conda clean -y --all
# cpanm git://github.com/IdoBar/XML-DOM-XPath-0.14@patch1
# cpanm --force Bio::SeqIO
# Install pdfx to parse the report and download the files, see https://stackoverflow.com/a/33173484
pip install pdfx
```

```{bash define_dirs}
REF_DIR=$HOME/data/reference_genomes/Ascochyta_reference_genomes # on Griffith HPC
# REF_DIR=$HOME/data/reference_genomes # on Awoonga HPC
WORK_DIR=$HOME/scratch/A_rabiei_WGS_2021 # on Griffith HPC
# WORK_DIR=$HOME/90days/data/A_rabiei_WGS_2021 # on Awoonga
```

```{bash prep_genome}
# download reference genomes
# source ~/.proxy # on Griffith HPC
wget -c ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/004/011/695/GCF_004011695.1_Arabiei_Me14/GCF_004011695.1_Arabiei_Me14_genomic.fna.gz -O $REF_DIR/GCF_004011695.1_Arabiei_Me14_genomic.fna.gz
wget -c ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/004/011/695/GCF_004011695.1_Arabiei_Me14/GCF_004011695.1_Arabiei_Me14_genomic.gff.gz -O $REF_DIR/GCF_004011695.1_Arabiei_Me14_genomic.gff.gz

# convert gff3 (from AUGUSTUS) and fasta files to genbank format
GENOME="$REF_DIR/GCF_004011695.1_Arabiei_Me14_genomic"
ls -1 $GENOME*.gz | parallel pigz -d {}
ln -s $GENOME.fna $GENOME.fa
genozip --make-reference $GENOME.fa
# cd $HOME/scratch/data/reference_genomes/Ascochyta_reference_genomes/
# download conversion script
# wget https://raw.githubusercontent.com/chapmanb/bcbb/master/gff/Scripts/gff/gff_to_genbank.py
# python gff_to_genbank.py $GENOME.gff3 $GENOME.fasta
```

Download the list of raw `.fastq.gz` files from AGRF and upload to CloudStor.
```{bash retrieve_files}
mkdir -p $WORK_DIR
cd !$
SSHUSER=NiloofarVaghefi1
SSHPASS=Ashtad3721
AGRF_DIR=files/AGRF_CAGRF20114478_H3HGFDSX2
# get the list of files from AGRF - doesn't work on Griffith HPC because of proxy settings - try using corkscrew (http://wiki.kartbuilding.net/index.php/Corkscrew_-_ssh_over_https)
sshpass -p $SSHPASS ssh $SSHUSER@agrf-data.agrf.org.au "ls -1 $AGRF_DIR" > AGRF_CAGRF20114478_H3HGFDSX2_files.list
# download md5 file
sshpass -p $SSHPASS rsync -avzh $SSHUSER@agrf-data.agrf.org.au:$AGRF_DIR/checksums.md5 ./
# rsync NiloofarVaghefi1@agrf-data.agrf.org.au:files/AGRF_CAGRF20114478_H3HGFDSX2/'*'
#rsync -avh -e ssh IdoBar1@agrf-data.agrf.org.au:files/AGRF_CAGRF19461_HGMLMAFXY ~/scratch/data/A_rabiei_WGS/

```

#### Adaptor Trimming
Adaptors needed to be removed, as well as very low quality bases/reads, so trimming was performed with `r fontFmt("fastp")` v0.20.1, which also produced QC figures and reports.  
Raw files quality after adaptor trimming was assessed with `r fontFmt("FastQC")` v0.11.8.

#### Mapping to the reference genome and calling variants (using Snippy)
The trimmed reads were mapped to the _A. rabiei_ reference genome, strain Me14 ([GCA_004011695.1](https://www.ncbi.nlm.nih.gov/assembly/GCA_004011695.1)) using `r fontFmt("Snippy")` v4.6.0. [Snippy](https://github.com/tseemann/snippy) is a wrapper that makes use of popular bioinformatics tools, such as `r fontFmt("bwa-mem")` v0.7.17-r1188 to map the reads to the reference genome, followed by several commands of `r fontFmt("samtools")` v1.12 to specify a Read Group for each sample (provided to Snippy with the `--rgid` flag), mark duplicates and convert the alignments into coordinate-sorted, indexed BAM files.  
The alignment files are then processed by `r fontFmt("Freebayes")` v1.3.5 to call variants from all samples and `r fontFmt("bcftools")` v1.12 and `r fontFmt("snpEff")` v5.0 to filter and annotate the variants and retain only high-quality variants (based on minimum depth and genotype quality thresholds) that are common to all samples and referred to as "core SNPs".  
Mapping statistics were obtained with `r fontFmt("Qualimap")` v.2.2.2-dev [@okonechnikov_qualimap_2016] and consolidated along with pre and post-trimming QC measures into a single, interactive report for each batch using `r fontFmt("MultiQC")` v1.10.1 [@ewels_multiqc:_2016]. 

*Processing of the files was done on Awoonga/Griffith HPC clusters*

```{bash trim_reads_snippy_AGRF}
# work on Awoonga (because Griffith HPC can't download the fastq files directly...)
CONDA_NAME=/export/home/s2978925/miniconda3/envs/snippy
conda activate $CONDA_NAME
BBMAP_REF=$(find $CONDA_PREFIX -wholename "*resources/adapters.fa")
# Prepare the BBduk commands
DATE=`date +%d_%m_%Y`
BATCH=AGRF
RUN="${BATCH}_snippy_${DATE}" # day of run was 02_02_2019
RUN_DIR=$WORK_DIR/${RUN}
mkdir -p $RUN_DIR
cd $RUN_DIR
# GENOME="$HOME/scratch/data/reference_genomes/Ascochyta_reference_genomes/A.rabiei_me14"
# READ_LENGTH=150
# PLOIDY=1
NCORES=12
RAM=28
# LANES=$( ls -1 ../*.fastq.gz | egrep -o "_L[0-9]+?_" | sort | uniq | wc -l )
RGPM=NovaSeq
RGPL=ILLUMINA
RGPU=H3HGFDSX2
RGCN=AGRF
COMMON_ID="DO16094321_HFKYMALXX"
case $RGPM in
    NextSeq )
        CLUMP_PARAM="dupedist=40 spany" ;;
    HiSeq2500 )
        CLUMP_PARAM="dupedist=40" ;;
    HiSeq3000 )
        CLUMP_PARAM="dupedist=2500" ;;
    HiSeq4000 )
        CLUMP_PARAM="dupedist=2500" ;;
    NovaSeq )
        CLUMP_PARAM="dupedist=12000" ;;
    * )
        CLUMP_PARAM="" ;;
esac
# where to store read files
# FQ_DIR="$RUN_DIR/fastq_files"
mkdir -p $RUN_DIR/CloudStor_copy
# Prepare a general array PBS script
echo '#!/bin/bash 
#PBS -V

cd $WORKDIR
>&2 echo Current working directory: $PWD
source ~/.bashrc'"
conda activate $CONDA_PREFIX 
set -Eeo pipefail
gawk -v ARRAY_IND=\$PBS_ARRAY_INDEX 'NR==ARRAY_IND' \$CMDS_FILE | bash" > ${WORK_DIR}/array.pbspro

# Then perform adapter trimming, mapping and variant calling (snippy); decide which intermediate files can be removed
# grep "_R1.fastq.gz" $WORK_DIR/AGRF_CAGRF20114478_H3HGFDSX2_files.list | sort | parallel -k --dry-run --rpl "{infile} s:_R1:_R#:; uq()" --rpl "{files} s:_R1:_R?:; uq()" --rpl "{sample} s:_H3HGFDSX2_[ACGT\-]+_L002_.+::"  "rclone copy CloudStor:Shared/GRIFFITH-Ford-Crop-Genetics-Lab/A_rabiei_WGS/AGRF_CAGRF20114478_H3HGFDSX2/AGRF_CAGRF20114478/ --include \"{files}\" ./{sample} ;  if [[ \$( md5sum {sample}/{files} | cut -d' ' -f1 ) == \$( egrep '{sample}_' $WORK_DIR/checksums.md5 | cut -d' ' -f1 )  ]]; then echo 'Files passed md5 check'; else exit 1 ; fi ; java -ea -XX:ParallelGCThreads=2 -Xmx4g -Xms4g -cp /home/ibar/.pyenv/versions/miniconda-latest/envs/snippy/opt/bbmap-38.90-1/current/ jgi.BBDuk ref=$BBMAP_REF ktrim=r k=23 mink=11 hdist=1 qtrim=rl trimq=10 tpe tbo minlen=30 ziplevel=8 threads=\$[NCPUS - 2] ow in={sample}/{infile} out={sample}/{sample}_R#.trimmed.fastq.gz stats={sample}.stats ow ; mkdir -p trimmed_reads/{sample}_QC ; fastqc -t \$[NCPUS - 2] -o trimmed_reads/{sample}_QC {sample}/{sample}*.trimmed.fastq.gz ; snippy --force --cleanup --cpus \$[NCPUS - 2] --ram $[RAM-2] --outdir snippy_{sample} --report --rgid {sample} --ref $GENOME.fna --R1 {sample}/{sample}_R1.trimmed.fastq.gz --R2 {sample}/{sample}_R2.trimmed.fastq.gz; rm -r {sample}/ " > $RUN_DIR/$RUN.cmds


# Then perform adapter trimming (fastp), mapping and variant calling (snippy); decide which intermediate files can be removed
# grep "_R1.fastq.gz" $WORK_DIR/AGRF_CAGRF20114478_H3HGFDSX2_files.list | sort | parallel --dry-run --rpl "{file2} s:_R1:_R2:; uq()" --rpl "{files} s:_R1:_R?:; uq()" --rpl "{sample} s:_H3HGFDSX2_[ACGT\-]+_L002_.+::"  "rclone copy CloudStor:Shared/GRIFFITH-Ford-Crop-Genetics-Lab/A_rabiei_WGS/AGRF_CAGRF20114478_H3HGFDSX2/AGRF_CAGRF20114478/ --include \"{files}\" ./{sample} ; cd {sample};  if [[ \$( egrep '{sample}_H3H' $WORK_DIR/checksums.md5 | md5sum -c | cut -f2 -d ' ' | uniq) == 'OK'  ]]; then >&2 echo 'Files passed md5 check'; else >&2 echo 'Files did not pass md5 check, please download again'; exit 99 ; fi ; fastp -i {} -I {file2} -c --adapter_fasta $BBMAP_REF -l 30 -p -w \$[NCPUS - 2] -z 7 -o {sample}_R1.trimmed.fastq.gz -O  {sample}_R2.trimmed.fastq.gz -h {sample}.fastp.html -j {sample}.fastp.json; fastqc -t \$[NCPUS - 2] -o {sample}_QC {sample}*.trimmed.fastq.gz ; snippy --force --cleanup --cpus \$[NCPUS - 2] --ram $[RAM-2] --outdir snippy_{sample} --report --rgid {sample} --ref $GENOME.fna --R1 {sample}_R1.trimmed.fastq.gz --R2 {sample}_R2.trimmed.fastq.gz" > $RUN_DIR/$RUN.cmds


# On Griffith HPC: perform adapter trimming (fastp), mapping and variant calling (snippy); decide which intermediate files can be removed
grep "_R1.fastq.gz" $WORK_DIR/AGRF_CAGRF20114478_H3HGFDSX2_files.list | sort | parallel -k --dry-run --rpl "{file2} s:_R1:_R2:; uq()" --rpl "{files} s:_R1:_R?:; uq()" --rpl "{sample} s:_H3HGFDSX2_[ACGT\-]+_L002_.+::"  "mkdir -p ./{sample} ;cd  {sample}; ln -sf $WORK_DIR/AGRF_CAGRF20114478/{files} ./;  if [[ \$( egrep '{sample}_H3H' $WORK_DIR/checksums.md5 | md5sum -c | cut -f2 -d ' ' | uniq) == 'OK'  ]]; then >&2 echo 'Files passed md5 check'; else >&2 echo 'Files did not pass md5 check, please download again'; exit 99 ; fi ; fastp -i {} -I {file2} -c --adapter_fasta $BBMAP_REF -l 30 -p -w \$[NCPUS - 2] -z 7 -o {sample}_R1.trimmed.fastq.gz -O {sample}_R2.trimmed.fastq.gz -h {sample}.fastp.html -j {sample}.fastp.json; snippy --force --cleanup --cpus \$[NCPUS - 2] --ram $[RAM-2] --outdir snippy_{sample} --report --rgid {sample} --ref $GENOME.fna --R1 {sample}_R1.trimmed.fastq.gz --R2 {sample}_R2.trimmed.fastq.gz " > $RUN_DIR/$RUN.cmds

# run just fastp trim
grep "_R1.fastq.gz" $WORK_DIR/AGRF_CAGRF20114478_H3HGFDSX2_files.list | sort | parallel -k --dry-run --rpl "{file2} s:_R1:_R2:; uq()" --rpl "{files} s:_R1:_R?:; uq()" --rpl "{sample} s:_H3HGFDSX2_[ACGT\-]+_L002_.+::"  "mkdir -p ./{sample} ;cd  {sample}; ln -sf $WORK_DIR/AGRF_CAGRF20114478/{files} ./;  if [[ \$( egrep '{sample}_H3H' $WORK_DIR/checksums.md5 | md5sum -c | cut -f2 -d ' ' | uniq) == 'OK'  ]]; then >&2 echo 'Files passed md5 check'; else >&2 echo 'Files did not pass md5 check, please download again'; exit 99 ; fi ; fastp -i {} -I {file2} -c --adapter_fasta $BBMAP_REF -l 30 -p -w \$[NCPUS - 2] -z 7 -o {sample}_R1.trimmed.fastq.gz -O {sample}_R2.trimmed.fastq.gz -h {sample}.fastp.html -j {sample}.fastp.json" > $RUN_DIR/fastp_trim.cmds


# find . -maxdepth 1 -name "*_R1.fastq.gz" | parallel --dry-run --rpl "{files} s:_R1:_R?:; uq()" --rpl "{sample} s:_H3HGFDSX2_[ACGT\-]+_L002_.+::"  "genozip {files} -f -@ \$[NCPUS - 2] -2 -E $GENOME.ref.genozip -o $RUN_DIR/CloudStor_copy/{sample}_H3HGFDSX2.fq.genozip; rm {files} trimmed_reads/{sample}_R?.recal.trimmed.fastq.gz" > genozip_clean.cmds

# GENOZIP_CLEAN=$(qsub -J1-$(cat $RUN_DIR/genozip_clean.cmds | wc -l) -l select=1:ncpus=6:mem=12GB,walltime=1:00:00 -N genozip_clean -A qris-gu -v "CMDS_FILE=$RUN_DIR/genozip_clean.cmds","WORKDIR=$RUN_DIR" ${WORK_DIR}/array.pbspro | egrep -o "^[0-9]+")
# recalibrate requires a reference 
# bbduk.sh in=stdin.fq out=stdout.fq int recalibrate | filterbytile.sh in={infile} out=stdout.fq int=f | clumpify.sh in=stdin.fq out=stdout.fq int dedupe usetmpdir optical $CLUMP_PARAM adjacent 

# add --cleanup to snippy
# make a folder for the log files
mkdir -p $RUN_DIR/pbs_logs
cd !$

# run fastp trim
FASTP_TRIM=$(qsub -J1-$(cat $RUN_DIR/fastp_trim.cmds | wc -l) -l select=1:ncpus=6:mem=8GB,walltime=1:00:00 -N fastp_trim -A qris-gu -v "CMDS_FILE=$RUN_DIR/fastp_trim.cmds","WORKDIR=$RUN_DIR" ${WORK_DIR}/array.pbspro | egrep -o "^[0-9]+")

# save failed commands to file
grep "job killed"  fastp_trim.e$FASTP_TRIM.* | awk -F [.:] '{print $3}' | awk 'NR==FNR{pos[$1]; next}FNR in pos' - $RUN_DIR/fastp_trim.cmds > $RUN_DIR/fastp_trim.cmds.$FASTP_TRIM.failed
# rerun failed jobs
cd pbs_logs
FASTP_TRIM=$(qsub -J1-$(cat $RUN_DIR/fastp_trim.cmds.$FASTP_TRIM.failed | wc -l) -l select=1:ncpus=8:mem=4GB,walltime=2:00:00 -N fastp_trim -A qris-gu -v "CMDS_FILE=$RUN_DIR/fastp_trim.cmds.$FASTP_TRIM.failed","WORKDIR=$RUN_DIR" ${WORK_DIR}/array.pbspro | egrep -o "^[0-9]+")

# test run
TEST_SNIPPY=$(qsub -J195-196 -l select=1:ncpus=${NCORES}:mem=${RAM}GB,walltime=5:00:00 -N ${RUN:0:11} -A qris-gu -v "CMDS_FILE=$RUN_DIR/$RUN.cmds","WORKDIR=$RUN_DIR" ${WORK_DIR}/array.pbspro | egrep -o "^[0-9]+")

# run for all files
STEP_SIZE=10

TRIM_SNIPPY=$(qsub -J1-$(cat $RUN_DIR/$RUN.cmds | wc -l) -l select=1:ncpus=${NCORES}:mem=${RAM}GB,walltime=5:00:00 -N ${RUN:0:11} -A qris-gu -v "CMDS_FILE=$RUN_DIR/$RUN.cmds","WORKDIR=$RUN_DIR" ${WORK_DIR}/array.pbspro | egrep -o "^[0-9]+")
#TRIM_SNIPPY2=$(qsub -J2-$(cat $RUN_DIR/$RUN.cmds | wc -l):$STEP_SIZE -l select=1:ncpus=${NCORES}:mem=${RAM}GB,walltime=1:00:00 -N ${RUN:0:11} -W depend=afterok:$TRIM_SNIPPY[] -A qris-gu -v "CMDS_FILE=$RUN_DIR/$RUN.cmds","WORKDIR=$RUN_DIR" ${WORK_DIR}/array.pbspro | egrep -o "^[0-9]+")

# check for successful commands and save them to a file
cd $RUN_DIR
egrep "ExitStatus:[0]" pbs_logs/*.e$TRIM_SNIPPY.* | awk -F [.:] '{print $3}' | awk 'NR==FNR{pos[$1]; next}FNR in pos' - $RUN.cmds > $RUN.$TRIM_SNIPPY.succeed
# check for failed commands and save them to a file
egrep "ExitStatus:[^0]" pbs_logs/*.e$TRIM_SNIPPY.* | awk -F [.:] '{print $3}' | awk 'NR==FNR{pos[$1]; next}FNR in pos' - $RUN.cmds > $RUN.$TRIM_SNIPPY.failed

# rerun failed commands (modify pbs file if needed to increase resources)
cd pbs_logs
TRIM_SNIPPY2=$(qsub -J1-$(cat $RUN_DIR/$RUN.$TRIM_SNIPPY.failed | wc -l) -l select=1:ncpus=${NCORES}:mem=${RAM}GB,walltime=5:00:00 -N ${RUN:0:11} -A qris-gu -v "CMDS_FILE=$RUN_DIR/$RUN.$TRIM_SNIPPY.failed","WORKDIR=$RUN_DIR" ${WORK_DIR}/array.pbspro | egrep -o "^[0-9]+")

# repeat as needed
egrep "ExitStatus:[^0]" pbs_logs/*.e$TRIM_SNIPPY2.* | awk -F [.:] '{print $3}' | awk 'NR==FNR{pos[$1]; next}FNR in pos' - $RUN.$TRIM_SNIPPY.failed > $RUN.$TRIM_SNIPPY2.failed
# rerun failed jobs
cd pbs_logs
TRIM_SNIPPY3=$(qsub -J1-$(cat $RUN_DIR/$RUN.$TRIM_SNIPPY2.failed | wc -l) -l select=1:ncpus=${NCORES}:mem=${RAM}GB,walltime=5:00:00 -N ${RUN:0:11} -A qris-gu -v "CMDS_FILE=$RUN_DIR/$RUN.$TRIM_SNIPPY2.failed","WORKDIR=$RUN_DIR" ${WORK_DIR}/array.pbspro | egrep -o "^[0-9]+")

# repeat as needed
egrep "ExitStatus:[^0]" pbs_logs/*.e$TRIM_SNIPPY3.* | awk -F [.:] '{print $3}' | awk 'NR==FNR{pos[$1]; next}FNR in pos' - $RUN.$TRIM_SNIPPY2.failed > $RUN.$TRIM_SNIPPY3.failed
# rerun failed jobs
cd pbs_logs
TRIM_SNIPPY_NEW=$(qsub -J1-$(cat $RUN_DIR/$NEW_RUN.bash | wc -l) -l select=1:ncpus=${NCORES}:mem=${RAM}GB,walltime=7:00:00 -N ${RUN:0:11} -A qris-gu -v "CMDS_FILE=$RUN_DIR/$NEW_RUN.bash","WORKDIR=$RUN_DIR" ${WORK_DIR}/array.pbspro | egrep -o "^[0-9]+")

# find incomplete jobs
cd $RUN_DIR
find . -name "snps.aligned.fa" | cut -f 3 -d / | gawk '
{printf "%s|", $1}' | sed 's/|$//' |  egrep -vf - $RUN_DIR/$RUN.cmds > $RUN_DIR/$RUN.cmds.failed
# rerun failed jobs
cd pbs_logs
TRIM_SNIPPY3=$(qsub -J1-$(cat $RUN_DIR/$RUN.cmds.failed | wc -l) -l select=1:ncpus=${NCORES}:mem=32GB,walltime=2:00:00 -N ${RUN:0:11} -v "CMDS_FILE=$RUN_DIR/$RUN.cmds.failed","WORKDIR=$RUN_DIR" ${WORK_DIR}/array.pbspro | egrep -o "^[0-9]+")

# find duplicate sample names that didn't work initially
egrep "AR0052_H3H|AR0039_H3H|AR0242_H3H" $RUN_DIR/$RUN.cmds > $RUN_DIR/$RUN.cmds.retry
TRIM_SNIPPY_RETRY=$(qsub -J1-$(cat $RUN_DIR/$RUN.cmds.retry | wc -l) -l select=1:ncpus=${NCORES}:mem=32GB,walltime=2:00:00 -N ${RUN:0:11} -v "CMDS_FILE=$RUN_DIR/$RUN.cmds.retry","WORKDIR=$RUN_DIR" ${WORK_DIR}/array.pbspro | egrep -o "^[0-9]+")

# remove empty log files
find . -type f -size 0 -name "${RUN:0:11}.*" -exec rm {} +

# Run snippy-core on the output files
cd $RUN_DIR
CORE_JOB=snippy_core_"$(date +%d_%m_%Y)"
SNPY_FOLDS=$( find . -type d -name "snippy_AR*" | xargs )
SNPY_REF="'$( find . -type d -name "snippy_AR*" | head -n1 )/reference/ref.fa'"

# CORE_CMD=$( tail -n1 ${RUN}.bash )
SNPY_CORE=$( echo "cd $( pwd ) ; source ~/.bashrc ; conda activate $CONDA_PREFIX ; snippy-core --prefix=CORE_JOB --ref $SNPY_REF $SNPY_FOLDS " | qsub -V -l select=1:ncpus=${NCORES}:mem=${RAM}GB,walltime=2:00:00  -N ${CORE_JOB:0:11} | egrep -o "[0-9]{7}" ) # 5248661.pbsserver


# cd $RUN_DIR
# find . -maxdepth 1 -type d -name "AR*" | cut -f2 -d"/" | parallel --dry-run "mkdir -p {}/{}_QC; cd {};  fastp -i {}*_R1.trimmed.fastq.gz -I {}*_R2.trimmed.fastq.gz -w $[NCPUS - 2] -h {}_QC/{}.fastp.html -j  {}_QC/{}.fastp.json -A -Q -G -m --stdout" > fastp_trim_qc.cmds

# FASTP_QC=$(qsub -J1-$(cat $RUN_DIR/fastp_trim_qc.cmds | wc -l) -l select=1:ncpus=${NCORES}:mem=${RAM}GB,walltime=5:00:00 -N ${RUN:0:11} -A qris-gu -v "CMDS_FILE=$RUN_DIR/fastp_trim_qc.cmds","WORKDIR=$RUN_DIR" ${WORK_DIR}/array.pbspro | egrep -o "^[0-9]+")
# record the array ID: 5247580[] Macrogen batch on Gowonda

# multiqc report
MULTIQC_JOB="${RUN}_QC"
echo "cd $( pwd ) ; multiqc -i $MULTIQC_JOB -o $MULTIQC_JOB ." | qsub -V -l select=1:ncpus=12:mem=4GB,walltime=1:00:00 -N ${MULTIQC_JOB:0:11} -W depend=afterok:$QUALIMAP_JOB_ID # 5248672.pbsserver

# collect trimming summary
grep "Result:" ${RUN}.log | gawk 'NR%2==0' | gawk 'BEGIN{per=0}{per+=gensub(/.([0-9]+\.[0-9]+).+/, "\\1", "1", $7); avg=per/NR}END{printf("Average percentage of bases kept after trimming: %.2f%% \n",avg)}' > $RUN.bbduk.stats
# Average percentage of bases kept after trimming: 97.76%
# Number of files processed: 72
grep "Input:" ${RUN}.log | gawk 'NR%2==0' | gawk 'BEGIN{reads=0};{reads+=$2; avg=reads/NR}END{printf("Average number of reads per file: %.2f \nTotal number of reads: %d\nNumber of files processed: %d\n",avg, reads,NR)}' >> $RUN.bbduk.stats
# Average number of reads per file: 4243748.83
# Total number of reads: 305549916
# find and remove empty files
find . -size 0 -exec rm {} + 
# Check that all jobs finished successfuly
find . -regextype posix-egrep -regex '\./.*\.e[0-9]{7}.*' | xargs grep "ExitStatus" #  *m.e$JOB_ID.*
# remove temp files
rm raw.sam raw_R*.fq.gz
# Done!
```

Samples with less than 20% mapping and x15 coverage were removed from the rest of the analysis (see highlighted rows in `r tbls(name="mapping_rates",display="cite")`).  

Error rates were estimated by individually processing the duplicate samples and assessing the rate of mismatch variant calls within replicates of the same isolate.
_To process mapping table, copy the `multisampleBamQcReport.html` file from the server to the local analysis folder_

```{r snippy_stats, eval=FALSE, echo=FALSE}
analysis_basename <- "A_rabiei_2018"
analysis_name <- "Snippy_multi_17_07_2019"
analysis_folder <- file.path("..", analysis_name)
variant_method <- "snippy"
# analysis_folder <- "../FB_vars_01_03_2019"
# analysis_outdir <- glue("./output/{variant_method}/")
error_rates <- readxl::read_excel("output/results/pipeline_comparison.xlsx", 
                                  sheet = "Snippy_multi_07_07_2019") %>% 
  select(-vcf_file)
mean_allele_error <- mean(error_rates$allele_error_rate)
stats_table <- read_tsv(file.path(analysis_folder,"core.txt")) 


# mapping_table <- list.files(analysis_folder, ".bbmap.stats", full.names = TRUE) %>% map_df(read_tsv) %>% 
#   mutate_at(vars(contains("mapped")),  ~as.numeric(sub("%", "", ., fixed=TRUE))/100) %>%
#   mutate(Mapping_rate=rowMeans(.[,-1]), Mapping_tool = "BBmap (v38.22)", 
#          Sample_id=str_extract(.$sample_id, paste(sequencing_table$Submission_id, collapse = "|"))) %>%
#   group_by(Sample_id, Mapping_tool) %>%
#   summarise(Mapping_rate=mean(Mapping_rate)) %>% ungroup() %>% 
#   left_join(sequencing_table,by=c("Sample_id"="Submission_id")) %>%
# # mutate( Isolate=sequencing_dict[sample_id]) %>% 
#   dplyr::select(Isolate, Mapping_rate, Mapping_tool, Sample_id, Sequencing_Centre) 

# Load qualimap statistics (copy summary html files to raw_data folder)

qualimap_files <- list.files(analysis_folder, "multisampleBamQcReport.html", 
                             full.names = TRUE, 
                             recursive = TRUE)
exclude_columns <- c("Coverage std", "Insert size median")
  # mutate(`Pathog. Group`=samples_table$Pathogenicity[match(samples_table$Isolate, .$Isolate)] ) 
qualimap_results <-  qualimap_files[1] %>% map_df(~htmltab(., which = "//td/b[text() = 'Sample name']/ancestor::table")) %>% 
  rename(Isolate=`Sample name`, Coverage=`Coverage mean`) %>% 
  dplyr::select(-one_of(exclude_columns)) %>% 
  mutate_at(.vars = vars(-starts_with("Isolate")), .funs = as.numeric) %>%
  left_join(., error_rates %>% select(Isolate=replicate_group, Error_rate=allele_error_rate,
                                       Replicates=replicate_num)) %>%
  arrange(Isolate, desc(Coverage)) %>% set_names(., gsub(" ", "_", colnames(.))) %>%
  # dplyr::select(Isolate, GC_percentage, Mapping_quality_mean, Mapping_rate, Coverage, Mapping_tool, Sample_id, Sequencing_Centre) %>%
  write_csv(filedate(glue::glue("{analysis_basename}_{analysis_name}.mapping.stats"), ".txt", "./output/results", dateformat = FALSE))

# Summarise mapping results
# mapping_summary <- qualimap_results %>% filter(Coverage>20) %>% 
#   group_by(Sequencing_Centre) %>% 
#   summarise(Coverage=sprintf("x%.2f",mean(Coverage)), 
#             Mapping_rate=sprintf("%.2f%%", mean(Mapping_rate, na.rm=TRUE)*100),
#                                      Mapping_qual=round(mean(Mapping_quality_mean), 2),
#             Files=n()) %>%
#   # mutate(Reads_per_file=format(c(2224642.64, 36500108.00), 
#   #                              digits = 2, scientific = TRUE)) %>%
#   arrange(desc(Sequencing_Centre)) %>% 
#   write_csv(filedate(glue::glue("{analysis_basename}_AGRF_Macro_AgVic.mapping.sum"), ".txt", "./output/results", dateformat = FALSE))

```

```{r map_stats, eval=FALSE}
datatable(as.data.frame(qualimap_results), # %>% mutate_if(is.numeric, ~round(., 2))),
          caption=tbls("mapping_rates")) %>% #, 
  formatRound(~Coverage+GC_percentage+Mapping_quality_mean) %>% 
  formatPercentage('Error_rate', 2) %>% 
          # options = list(dom = 'tf', pageLength = 40)) %>%
  formatStyle("Coverage", target = "row",
  backgroundColor = styleInterval(c(10,20), c('#ef3125', '#feed95', NA))
)

# pander(as.data.frame(qualimap_results), caption=tbls("mapping_rates"), justify="left")

# datatable(as.data.frame(mapping_stats), caption = tbls("mapping_rates"), # , width=10
#           style="default", rownames = FALSE, autoHideNavigation=TRUE) %>% 
#   formatPercentage('Mapping_rate', 2) %>% #formatStyle('Isolate',
#   formatStyle('Mapping_rate',
#     color = styleInterval(c(0.05, 0.5), c('yellow', 'blue', 'black')),
#     backgroundColor = styleInterval(c(0.05, 0.5), c('red', 'yellow',NA))
#   )
```



```{r QC_GC, eval=FALSE, echo=FALSE, out.width='100%', fig.cap=figs("GC_cont")}
include_graphics("plots/fastqc_per_sequence_gc_content_plot.png")
```

### Population Genetics
The core SNPs were imported to R and were used to 


### Variant Annotation
Variants were then further filtered to keep only polymorphic and bi-allelic SNPs using the `r fontFmt("VariantAnnotation")` v`r packageVersion("VariantAnnotation")` and `r fontFmt("GenomicRanges")` v`r packageVersion("GenomicRanges")` R Bioconductor packages [@obenchain_variantannotation:_2014; @lawrence_software_2013]. The SNPs were annotated and position of each variant relative to gene locations was determined (coding/intron/promotor/intergenic), based on the reference genome and gene models of _A. rabiei_ strain _Me14_ (where the contig names were shortened to remove the `Arab_Me14` prefix). 

```{bash fix_contig_names}
cat A.rabiei_me14.fasta | sed 's/Arab_Me14_//' > A_rabiei_me14_short_names.fasta
zcat Arab_me14.gff3.gz | sed 's/Arab_Me14_//' > Arab_me14_short_names.gff
```

Variants in coding regions of genes were identified as Synonymous/Non-synonymous/Non-sense mutations if they were silent, changing the amino acid or the reading frame, respectively. Additional annotation of the genes at each SNP site was performed by a BLASTp search against the NCBI non-redundant protein database (nr) and scanning against the InterPro conglomerate dbs. Effector prediction of the variant-associated genes was performed by `r fontFmt("EffectorP")` v1.0/2.0 [@SperschneiderEffectorPpredictingfungal2016;@JanaImprovedpredictionfungal2018]. The variants were summarised and visualised across the genome scaffolds and visualised using `r fontFmt("circlize")` v`r packageVersion("circlize")` R package [@gu_circlize_2014].

```{bash interproscan}
# Download the python wrapper

$HPC_GRID_RUNNER_DIR/BioIfx/hpc_FASTA_GridRunner.pl --cmd_template "cd $PBS_O_WORKDIR; pyenv shell miniconda3-latest; ~/etc/tools/Annotation/InterPro/iprscan5_urllib3.py --sequence=__QUERY_FILE__ --outfile=__QUERY_FILE__.interpro.tsv --outformat=tsv --goterms --pathways --email=i.bar@griffith.edu.au " --query_fasta Assoc_SNP_genes_cds_05_02_2018.fasta -G $HPC_GRID_RUNNER_DIR/hpc_conf/small_PBS_jobs.conf -N 1 -O SNP_interpro
```

Look for particular effector genes and genes associated with plant-pathogen interactions and pathogenicity genes. Look in the literature and create a list of target genes.

### Variant-Pathogenicity association
Association between variants and pathogenicity levels was identified by `r fontFmt("SNPassoc")` v`r packageVersion("SNPassoc")` R package [@gonzalez_snpassoc:_2007], using a codominant gene model and a significance threshold of _p_-value $\le0.005$.  



## Appendix 2. Useful resources

* Whole-Genome Comparison of _Aspergillus fumigatus_ Strains Serially Isolated from Patients with Aspergillosis. [@hagiwara_whole-genome_2014]:

> **Sequence analysis:** The Illumina data sets were trimmed using fastq-mcf in ea-utils (version 1.1.2-484), i.e., sequencing adapters and sequences with low quality scores (Phred score [Q], <30) were removed (24). The data sets were mapped to the genome sequence of the _A. fumigatus_ genome reference strain Af293 (29,420,142 bp, genome version s03-m04-r03) (25, 26) using Bowtie 2 (version 2.0.0-beta7) with the very sensitive option in end-to-end mode (27). Duplicated reads were removed using Picard (version 1.112) (<http://picard.sourceforge.net>). The programs mpileup and bcftools from SAMtools (version 0.1.19-44428cd) were used to perform further quality controls. In mpileup, the -q20 argument was used to trim reads with low-quality mapping, whereas the argument -q30 was used to trim low-quality bases at the 3' end (28). The bcftools setting was set to -c in order to call variants using Bayesian inference. Consensus and single nucleotide polymorphisms (SNPs) were excluded if they did not meet a minimum coverage of 5x or if the variant was present in <90% of the base calls (29, 30). The genotype field in the variant call format (VCF) files indicates homozygote and heterozygote probabilities as Phred-scaled likelihoods. SNPs were excluded if they were called as heterozygous genotypes using SAMtools. The mapping results were visualized in the Integrative Genomics Viewer (version 2.3.3) (31, 32). The reference genome data included information on open reading frames and annotations, from which the SNPs were designated non-synonymous or synonymous.  
Single nucleotide mutations were confirmed by Sanger sequencing. Regions of approximately 400 bp that contained a mutation were amplified with appropriately designed primer pairs and then sequenced. The primer sequences are listed in Table S1 in the supplemental material, which were named as follows. For verification of the SNPs in strains from patient I or patient II, PaI or PaII was added to the primer name, respectively. For non-synonymous SNPs, synonymous SNPs, or SNPs in a non-coding region, (NS, Syno, NonC) was added to the primer name, respectively.  
**Analysis of unmapped reads:** _De novo_ assembly of the unmapped reads was conducted using the Newbler assembler 2.9 (Roche), with default parameters. The contigs were selected based on size/depth criteria: those of <500 bp and/or with a depth of <30x coverage were removed. To investigate whether unique genome sequences were present in strains isolated from the same patient, the unmapped reads of each strain were mapped to the contigs generated from all the strains in the same patient by the Bowtie 2 software. The coverage of the mapped regions was then evaluated. Gene predictions were performed using the gene prediction tool AUGUSTUS (version 2.5.5), with a training set of  _A. fumigatus_ (33). The parameters of AUGUSTUS were -species = aspergillus_fumigatus, -strand = both, -genemodel = partial, -singlestrand = false, -protein = on, -introns = on, -start = on, -stop = on, -cds = on, and -gff3 = on. To compare all the predicted genes with _Aspergillus_ genes, consisting of 244,811 genes available on AspGD (34), a reciprocal BLAST best hit approach was performed by BLASTp (35), with an E value of 1.0e<sup>-4</sup>. All BLASTp results were filtered based on a BLASTp identity of $\ge 80$% and an aligned length coverage of $\ge 80$%.

## General information
This document was last updated at `r Sys.time()` using R Markdown (built with `r R.version.string`). Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. It is especially powerful at authoring documents and reports which include code and can execute code and use the results in the output. For more details on using R Markdown see <http://rmarkdown.rstudio.com> and [Rmarkdown cheatsheet](https://www.rstudio.com/wp-content/uploads/2016/03/rmarkdown-cheatsheet-2.0.pdf).

***
## Bibliography

<!-- ```{r results='asis', eval=TRUE} -->
<!-- PrintBibliography(biblio) -->
<!-- ``` -->

